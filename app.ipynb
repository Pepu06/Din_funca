{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\47572859\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify \n",
    "import os\n",
    "import re\n",
    "import pypdf\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "import google.generativeai as palm\n",
    "import google.generativeai as genai\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from typing import List\n",
    "import speech_recognition as sr\n",
    "import json\n",
    "\n",
    "os.environ[ \"GEMINI_API_KEY\" ]= \"AIzaSyASfBmk76PK3hpBQ0Bjp_ACA3ineq53VvM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Reads the text content from a PDF file and returns it as a single string.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The file path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    - str: The concatenated text content of all pages in the PDF.\n",
    "    \"\"\"\n",
    "    # Logic to read pdf\n",
    "    reader = PdfReader(file_path)\n",
    "\n",
    "    # Loop over each page and store it in a variable\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "\n",
    "    return text\n",
    "\n",
    "# replace the path with your file path\n",
    "pdf_text = load_pdf(file_path=\"mati.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text: str):\n",
    "    \"\"\"\n",
    "    Splits a text string into a list of non-empty substrings based on the specified pattern.\n",
    "    The \"\\n \\n\" pattern will split the document para by para\n",
    "    Parameters:\n",
    "    - text (str): The input text to be split.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: A list containing non-empty substrings obtained by splitting the input text.\n",
    "\n",
    "    \"\"\"\n",
    "    split_text = re.split('\\n \\n', text)\n",
    "    return [i for i in split_text if i != \"\"]\n",
    "\n",
    "chunked_text = split_text(text=pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "    \"\"\"\n",
    "    Custom embedding function using the Gemini AI API for document retrieval.\n",
    "\n",
    "    This class extends the EmbeddingFunction class and implements the __call__ method\n",
    "    to generate embeddings for a given set of documents using the Gemini AI API.\n",
    "\n",
    "    Parameters:\n",
    "    - input (Documents): A collection of documents to be embedded.\n",
    "\n",
    "    Returns:\n",
    "    - Embeddings: Embeddings generated for the input documents.\n",
    "    \"\"\"\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "        if not gemini_api_key:\n",
    "            raise ValueError(\"Gemini API Key not provided. Please provide GEMINI_API_KEY as an environment variable\")\n",
    "        genai.configure(api_key=gemini_api_key)\n",
    "        model = \"models/embedding-001\"\n",
    "        title = \"Custom query\"\n",
    "        return genai.embed_content(model=model,\n",
    "                                   content=input,\n",
    "                                   task_type=\"retrieval_document\",\n",
    "                                   title=title)[\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chroma_db(documents:List, path:str, name:str):\n",
    "    \"\"\"\n",
    "    Creates a Chroma database using the provided documents, path, and collection name.\n",
    "\n",
    "    Parameters:\n",
    "    - documents: An iterable of documents to be added to the Chroma database.\n",
    "    - path (str): The path where the Chroma database will be stored.\n",
    "    - name (str): The name of the collection within the Chroma database.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[chromadb.Collection, str]: A tuple containing the created Chroma Collection and its name.\n",
    "    \"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=path)\n",
    "    db = chroma_client.create_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n",
    "\n",
    "    for i, d in enumerate(documents):\n",
    "        db.add(documents=d, ids=str(i))\n",
    "\n",
    "    return db, name\n",
    "\n",
    "db,name =create_chroma_db(documents=chunked_text, \n",
    "                          path=\"RAG\", #replace with your path\n",
    "                          name=\"rag_experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chroma_collection(path, name):\n",
    "    \"\"\"\n",
    "    Loads an existing Chroma collection from the specified path with the given name.\n",
    "\n",
    "    Parameters:\n",
    "    - path (str): The path where the Chroma database is stored.\n",
    "    - name (str): The name of the collection within the Chroma database.\n",
    "\n",
    "    Returns:\n",
    "    - chromadb.Collection: The loaded Chroma Collection.\n",
    "    \"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=path)\n",
    "    db = chroma_client.get_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n",
    "\n",
    "    return db\n",
    "\n",
    "db=load_chroma_collection(path=\"RAG\", name=\"rag_experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    }
   ],
   "source": [
    "def get_relevant_passage(query, db, n_results):\n",
    "  passage = db.query(query_texts=[query], n_results=n_results)['documents'][0]\n",
    "  return passage\n",
    "\n",
    "#Example usage\n",
    "relevant_text = get_relevant_passage(query=\"Intellykeys\",db=db,n_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escuchar_mic():\n",
    "    # recognizer = sr.Recognizer()\n",
    "    # with sr.Microphone() as source:\n",
    "    #     print(\"Escuchando...\")\n",
    "    #     try:\n",
    "    #         audio = recognizer.listen(source, phrase_time_limit=5)\n",
    "    #         texto = recognizer.recognize_google(audio, language=\"es-ES\")\n",
    "    #         print(f\"Texto reconocido: {texto}\")\n",
    "    #         return texto\n",
    "    #     except sr.UnknownValueError:\n",
    "    #         print(\"No se pudo reconocer el audio.\")\n",
    "    #         return None\n",
    "    #     except sr.RequestError as e:\n",
    "    #         print(f\"Error en la solicitud de reconocimiento: {e}\")\n",
    "    #         return None\n",
    "    texto = \" Soy Pedro\"\n",
    "    return texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_rag_prompt(query, texto_mic, relevant_passage):\n",
    "  escaped_passage = relevant_passage.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\n\", \" \")\n",
    "  escaped_texto_mic = texto_mic.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\n\", \" \")\n",
    "  prompt = (\"\"\"You are a helpful SPANISH and knowledgeable assistant that helps complete text using the reference passage and microphone context included below. \\\n",
    "  Ensure your response is fluent, coherent, and relevant to the user's text, adding value and depth where appropriate. \\\n",
    "  You are assisting a general audience, so focus on clarity, providing thoughtful and well-structured continuations or expansions of the given text. Maintain a friendly, conversational tone. \\\n",
    "  Give considerable importance to the context provided by the microphone. Generate ALWAYS three options of the completed text for the user to choose from. Each one should be unique and different from the others and not much longer than the user's text. \\\n",
    "  If the passage does not provide relevant information, you may complete the user's text based on your understanding. \\\n",
    "  DO NOT FORGET TO INCLUDE THE USER TEXT IN THE BEGINNING OF THE COMPLETION. \\\n",
    "          \n",
    "  USER TEXT: '{query}'\n",
    "  PASSAGE: '{escaped_passage}'\n",
    "  MICROPHONE CONTEXT: '{escaped_texto_mic}'\n",
    "\n",
    "  COMPLETION:\n",
    "  \"\"\").format(query=query, escaped_passage=escaped_passage, escaped_texto_mic=escaped_texto_mic)\n",
    "\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "def generate_answer_by_prompt(prompt):\n",
    "    gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not gemini_api_key:\n",
    "        raise ValueError(\"Gemini API Key not provided. Please provide GEMINI_API_KEY as an environment variable\")\n",
    "    genai.configure(api_key=gemini_api_key)\n",
    "    model = genai.GenerativeModel('gemini-pro')\n",
    "    answer = model.generate_content(prompt)\n",
    "    return answer.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(db,query):\n",
    "    #retrieve top 3 relevant text chunks\n",
    "    texto_mic = escuchar_mic()\n",
    "    relevant_text = get_relevant_passage(query,db,n_results=3)\n",
    "    prompt = make_rag_prompt(query, \n",
    "                             texto_mic,\n",
    "                             relevant_passage=\"\".join(relevant_text)) # joining the relevant chunks to create a single passage\n",
    "    answer = generate_answer_by_prompt(prompt)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 - 'Hola Pedro, me llamo Matías Duncan Federico. Desde chico uso un dispositivo que se llama IntellyKeys USB para comunicarme (el nombre es importante y ya van a ver porqué).'\n",
      "  2 - 'Hola Pedro, soy Matías. Me comunico mediante un software que utilizo con mis pies.'\n",
      "  3 - 'Hola Pedro, mi nombre es Matías Duncan Federico. Me comunico con un dispositivo que se llama IntellyKeys USB, lo uso con mis pies.'\n"
     ]
    }
   ],
   "source": [
    "db=load_chroma_collection(path=\"RAG\", #replace with path of your persistent directory\n",
    "                          name=\"rag_experiment\") #replace with the collection name\n",
    "\n",
    "answer = generate_answer(db,\"Hola, \")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Collection' object has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m db \u001b[38;5;241m=\u001b[39m load_chroma_collection(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAG\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrag_experiment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Evalúa el modelo\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecisión del modelo: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[27], line 20\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(test_data, db)\u001b[0m\n\u001b[0;32m     17\u001b[0m expected_answer \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Genera la respuesta usando tu modelo\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m generated_answer \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Añade las respuestas esperadas y generadas a las listas\u001b[39;00m\n\u001b[0;32m     23\u001b[0m y_true\u001b[38;5;241m.\u001b[39mappend(expected_answer)\n",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m, in \u001b[0;36mgenerate_answer\u001b[1;34m(model, prompt)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_answer\u001b[39m(model, prompt: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m(prompt)\n",
      "File \u001b[1;32mc:\\Users\\47572859\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pydantic\\main.py:828\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[1;32m--> 828\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Collection' object has no attribute 'generate'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define un conjunto de datos de prueba\n",
    "test_data = [\n",
    "    {\"query\": \"Hola, \", \"expected_answer\": \"Hola, ¿cómo puedo ayudarte?\"},\n",
    "    {\"query\": \"¿Cómo estás?\", \"expected_answer\": \"Estoy bien, gracias.\"},\n",
    "    {\"query\": \"¿Qué hora es?\", \"expected_answer\": \"Son las 3 PM.\"}\n",
    "]\n",
    "\n",
    "# Genera respuestas y evalúa la precisión\n",
    "def evaluate_model(test_data, db):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for data in test_data:\n",
    "        query = data[\"query\"]\n",
    "        expected_answer = data[\"expected_answer\"]\n",
    "        \n",
    "        # Genera la respuesta usando tu modelo\n",
    "        generated_answer = generate_answer(db, query)\n",
    "        \n",
    "        # Añade las respuestas esperadas y generadas a las listas\n",
    "        y_true.append(expected_answer)\n",
    "        y_pred.append(generated_answer)\n",
    "    \n",
    "    # Calcula la precisión\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Carga la colección Chroma\n",
    "db = load_chroma_collection(path=\"RAG\", name=\"rag_experiment\")\n",
    "\n",
    "# Evalúa el modelo\n",
    "accuracy = evaluate_model(test_data, db)\n",
    "print(f'Precisión del modelo: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
